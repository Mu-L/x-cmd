# shellcheck shell=dash
xrc chat
xrc:mod:lib     ollama      chat/send

___x_cmd_ollama_chat(){
    local X_help_cmd='x help -m ollama chat'; help:arg:parse
    local op="$1";
    case "$op" in
        request|exec)
            shift; ___x_cmd_ollama_chat_"$op" "$@" ;;
        *)  N=ollama M="Not support such option '$op'" log:ret:64
    esac
}

___x_cmd_ollama_chat_request(){
    local X_help_cmd='x help -m ollama chat request'; help:arg:parse

    ___x_cmd_ollama_is_installed || return
    ___x_cmd chat --exec --provider ollama "$@"
}

___x_cmd_ollama_chat_exec(){
    [ -n "$model" ] || {
        ___x_cmd_ollama_cur model:=  2>/dev/null
        [ -n "$model" ] || model="llama3"
    }

    ___x_cmd_ollama_model_has_model "$model" || {
        if ! ___x_cmd_is_interactive; then
            ollama:error --cmd "x ollama model download $model" "Please download the model in advance"
        else
            x ui yesno "Not found [MODEL ==> $model]. Do you want to install $model ?" || return 1
            ___x_cmd_ollama_model_download "$model" || return
        fi
    }

    local cfg_model=;
    ___x_cmd_ollama_cfg --var cfg_model=model  2>/dev/null
    ollama:debug --cfg_model "$cfg_model" --model "$model" "chat exec"

    {
        userlang="${cfg_userlang:-$LANG}" \
        BODY="$question" \
        x cawk  -E ___X_CMD_CHAT_SESSION_DIR,session,history_num,minion,model,chatid,minion_json_cache,system,type,filelist_attach   \
                -E cfg_history_num,cfg_session,cfg_minion,cfg_model,cfg_maxtoken,cfg_seed,cfg_temperature \
                -m j/json,j/jiter,j/jcp \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/history.awk"        \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/util.awk"           \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/minion.awk"         \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/creq.awk"           \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/cres.awk"           \
                -f "$___X_CMD_ROOT_MOD/ollama/lib/awk/ollama.awk"       \
                -f "$___X_CMD_ROOT_MOD/ollama/lib/awk/handle_request.awk"
    } | ( ___x_cmd_ollama_chat_request___launch )
}

___x_cmd_ollama_chat_request___launch(){
    local content_dir
    read -r content_dir

    ___x_cmd_ollama_chat_request___trapexit(){
        x rmrf "$content_dir/chat.running"
    }

    printf "%s\n" $$ >"$content_dir/chat.running"
    trap '___x_cmd_ollama_chat_request___trapexit' EXIT

    {
        read -r model

        local requestbody
        read -r requestbody

        x retry --max 2 --interval 3 ___x_cmd_ollama_chat_request___try
    } || {
        ___x_cmd_ollama_chat_request___trapexit
        return 1
    }
}

___x_cmd_ollama_chat_request___try(){
    [ -z "$confirm_before_send" ] || {
        printf "%s\n" "$requestbody" | x j2y | x bat -l yml >/dev/tty
        x ui yesno "Do your want to send this message?" || return 0
    }

    {
        ___x_cmd ccmd "$cache_time" -- \
            ___x_cmd_ollama_api_chat "$requestbody" "$model"
    } | {
        local interative=
        if ___x_cmd_is_interactive; then interative=1; fi

        local errcode=0
        x cawk  -E content_dir,interative         \
                -m j/json,j/jiter,j/jcp                                     \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/util.awk"               \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/minion.awk"             \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/cres.awk"               \
                -f "$___X_CMD_ROOT_MOD/ollama/lib/awk/ollama.awk"           \
                -f "$___X_CMD_ROOT_MOD/ollama/lib/awk/ollama_stream_output_util.awk"    \
                -f "$___X_CMD_ROOT_MOD/ollama/lib/awk/handle_response.awk"
    }
}

