# shellcheck shell=dash disable=SC2034

xrc cfgy
___x_cmd_ollama_cfg(){
    [ "$#" -gt 0 ] || {
        ___x_cmd_ollama_init
        return
    }

    local op="$1"; shift
    case "$op" in
        -h|--help)      x help -m ollama --cfg ;;
        *)              ___x_cmd_ollama_cfg___invoke "$op" "$@" ;;
    esac
}

___x_cmd_ollama_cur(){
    local X_help_cmd=; X_help_cmd='x help -m ollama --cur' help:arg:parse
    ___x_cmd_ollama_cfg --current "$@"
}

___x_cmd_ollama_init(){
    local cur_apikey=; local cur_model=; local cur_maxtoken=; local cur_proxy=; local cur_endpoint=;

    if [ -f "$(___x_cmd_ollama_cur --get config)" ]; then
        ___x_cmd_ollama_cur cur_apikey:=apikey cur_model:=model cur_maxtoken:=maxtoken cur_proxy:=proxy cur_endpoint:=endpoint 2>/dev/null
    fi
    
    ___x_cmd_ollama_cfg___invoke --init ${cur_apikey:+"--ctrl_exit_strategy"}                                   \
        model       "Set the model of ollama ai"                                                                \
                    "${cur_model:-"mistral"}"   '=' llama2 mistral dolphin-phi phi neural-chat         \
                                                    starling-lm codellama llama2-uncensored llama2:13b \
                                                    llama2:70b orca-mini vicuna llava gemma:2b gemma:7b
}

# Config setting the key.
# set the default prompt

___X_CMD_OLLAMA_CFG_VARLIST="apikey,model,maxtoken,proxy,seed,temperature,endpoint"
___x_cmd_ollama_cfg___invoke(){
    ___x_cmd_cfgy_obj                                                \
        --prefix            ___X_CMD_OLLAMA_CFG_DATA                \
        --default-config    "${___X_CMD_ROOT_CFG}/ollama/X.cfg.yml" \
        --current-config    "${___X_CMD_OLLAMA_LOCAL_CONFIG}"       \
        --current-profile   "${___X_CMD_OLLAMA_LOCAL_PROFILE}"      \
        --varlist           "$___X_CMD_OLLAMA_CFG_VARLIST"          \
        "$@"
}

