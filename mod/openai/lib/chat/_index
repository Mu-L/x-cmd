# this is a moduel to honor x-bash/chat module

xrc chat
xrc:mod:lib     openai      chat/send

___x_cmd_openai_chat(){
    local op="$1";
    case "$op" in
        request|preparehistory)
            shift; ___x_cmd_openai_chat_"$op" "$@" ;;
        *)  N=openai M="Not support such option '$op'" log:ret:64
    esac
}

___x_cmd_openai_chat_get_record(){
    :
}

___x_cmd_openai_chat_request(){
    local question=""
    local history_num=""
    local minion=""
    local session=""
    local model=""

    while [ $# -gt 0 ]; do
        case "$1" in
            --session)      session="$2";           shfit 2 ;;
            --minion)       minion="$2";            shift 2 ;;
            --model)        model="$2";             shift 2 ;;
            --history)      history_num="$2";       shift 2 ;;
            --question)     question="$2";          shift 2 ;;
            *)              question="$*";          break ;;
        esac
    done

    local cfg_history_num=; local cfg_session=;     local cfg_minion=
    ___x_cmd_chat_cfg --var cfg_history_num=history cfg_session=session cfg_minion=minion 2>/dev/null

    local cfg_model=;       local cfg_maxtoken=;    local cfg_seed=;    local cfg_temperature=
    ___x_cmd_openai_cfg --var cfg_model=model cfg_maxtoken=maxtoken cfg_seed=seed cfg_temperature=temperature 2>/dev/null

    local chatid=;  chatid="$(x pidid vlseqid ___X_CMD_CHAT_REC)"

    [ -n "$minion" ] || minion="default"
    local x_=; ___x_cmd_chat_minion_cache_json_ "$minion" || return
    local minion_json_cache="$x_"

    openai:debug --chatid "$chatid" --model "$model" --question "$question" "chat request"

    {
        x cawk  -E ___X_CMD_CHAT_SESSION_DIR,session,history_num,minion,model,question,chatid,minion_json_cache   \
                -E cfg_history_num,cfg_session,cfg_minion,cfg_model,cfg_maxtoken,cfg_seed,cfg_temperature \
                -m j/json,j/jiter,j/jcp \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/history.awk"        \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/util.awk"           \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/minion.awk"         \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/creq.awk"           \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/cres.awk"           \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai.awk"       \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/handle_request.awk"
    } | {
        read -r model

        local requestbody
        read -r requestbody

        ___x_cmd_openai_request_generaxwtecontent "$requestbody" "$model" | \
        {
            local interative=
            if ___x_cmd_is_interactive; then interative=1; fi
            x cawk  -E session,interative,minion_json_cache,chatid,___X_CMD_CHAT_SESSION_DIR,cfg_session         \
                    -m j/json,j/jiter,j/jcp                                     \
                    -f "$___X_CMD_ROOT_MOD/chat/lib/awk/util.awk"               \
                    -f "$___X_CMD_ROOT_MOD/chat/lib/awk/minion.awk"             \
                    -f "$___X_CMD_ROOT_MOD/chat/lib/awk/cres.awk"               \
                    -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai.awk"           \
                    -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai_stream_output_util.awk"    \
                    -f "$___X_CMD_ROOT_MOD/openai/lib/awk/handle_response.awk"
        }
   }
}

___x_cmd_openai_chat_request_with_pipe(){
    :
}

# x openai chat --model gpt4 -n 3 --file a.md -f b.md -f c.md --prompt english ''

# @gpt3 -n 3 -f a.md -f b.md -p en --
# @gpt3 -n 3 -f a.md -f c.jd -p cn --
# @gpt4

# using control command for this like vscode

# x chat start

# x chat start ==> create a new thread
# x chat set 3 ==> set history to 3
# @gpt3 how to understand

# @ws /start
# @ws /set 3

# x chat history cp ''

# @en
# @cn

