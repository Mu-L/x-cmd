# this is a moduel to honor x-bash/chat module

xrc chat
xrc:mod:lib     openai      chat/send

___x_cmd_openai_chat(){
    local op="$1";
    case "$op" in
        request|preparehistory)
            shift; ___x_cmd_openai_chat_"$op" "$@" ;;
        *)  N=openai M="Not support such option '$op'" log:ret:64
    esac
}

___x_cmd_openai_chat_get_record(){
    :
}

___x_cmd_openai_chat_request(){
    local question=""
    local history_num=""
    local minion=""
    local session=""
    local model=""
    local system=""
    local type="chat"
    local confirm_before_send=""

    while [ $# -gt 0 ]; do
        case "$1" in
            --session)      session="$2";           shfit 2 ;;
            --minion)       minion="$2";            shift 2 ;;
            --history|-n)   history_num="$2";       shift 2 ;;
            --question)     question="$2";          shift 2 ;;
            --model)        model="$2";             shift 2 ;;
            --system)       system="$2";            shift 2 ;;
            --type)         type="$2";              shift 2 ;;
            -c)             confirm_before_send=1;  shift 1 ;;
            *)              question="${question}$*"; break ;;
        esac
    done

    local cfg_history_num=; local cfg_session=;     local cfg_minion=
    ___x_cmd_chat_cfg --var cfg_history_num=history cfg_session=session cfg_minion=minion 2>/dev/null

    local cfg_model=;       local cfg_maxtoken=;    local cfg_seed=;    local cfg_temperature=
    ___x_cmd_openai_cfg --var cfg_model=model cfg_maxtoken=maxtoken cfg_seed=seed cfg_temperature=temperature 2>/dev/null

    local chatid=;  chatid="$(x pidid vlseqid ___X_CMD_CHAT_REC)"

    [ -n "$minion" ] || minion="default"
    local x_=; ___x_cmd_chat_minion_cache_json_ "$minion" || return
    local minion_json_cache="$x_"

    openai:debug --chatid "$chatid" --model "$model" --question "$question" --history_num "$history_num" --confirm_before_send "$confirm_before_send" "chat request"

    {
        x cawk  -E ___X_CMD_CHAT_SESSION_DIR,session,history_num,minion,model,question,chatid,minion_json_cache,system,type   \
                -E cfg_history_num,cfg_session,cfg_minion,cfg_model,cfg_maxtoken,cfg_seed,cfg_temperature \
                -m j/json,j/jiter,j/jcp \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/history.awk"        \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/util.awk"           \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/minion.awk"         \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/creq.awk"           \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/cres.awk"           \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai.awk"       \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/handle_request.awk"
    } | {
        local content_dir
        read -r content_dir

        ___x_cmd_openai_chat_request___trapexit(){
            openai:debug "Remove chat.running file"
            x rmrf "$content_dir/chat.running"
        }

        printf "%s\n" $$ >"$content_dir/chat.running"
        trap '___x_cmd_openai_chat_request___trapexit' EXIT

        {
            read -r model

            local requestbody
            read -r requestbody

            x retry --max 2 --interval 3 ___x_cmd_openai_chat_request___try
        } || {
            ___x_cmd_openai_chat_request___trapexit
            return 1
        }

   }
}

___x_cmd_openai_chat_request___try(){
    openai:debug "Sending request to openai server"
    [ -z "$confirm_before_send" ] || {
        printf "%s\n" "$requestbody" | x j2y | x bat -l yml
        x ui yesno "Do your want to send this message?" || return 0
    }

    ___x_cmd_openai_request_generaxwtecontent "$requestbody" "$model" | \
    {
        local interative=
        if ___x_cmd_is_interactive; then interative=1; fi

        local errcode=0
        x cawk  -E content_dir,interative         \
                -m j/json,j/jiter,j/jcp                                     \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/util.awk"               \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/minion.awk"             \
                -f "$___X_CMD_ROOT_MOD/chat/lib/awk/cres.awk"               \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai.awk"           \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/openai_stream_output_util.awk"    \
                -f "$___X_CMD_ROOT_MOD/openai/lib/awk/handle_response.awk" || errcode=$?

        case $errcode in
            0)      ;;
            *)  x rmrf "$content_dir/openai.response.yml" "$content_dir/chat.response.yml" ;;
        esac
        return "$errcode"
    }
}

# x openai chat --model gpt4 -n 3 --file a.md -f b.md -f c.md --prompt english ''

# @gpt3 -n 3 -f a.md -f b.md -p en --
# @gpt3 -n 3 -f a.md -f c.jd -p cn --
# @gpt4

# using control command for this like vscode

# x chat start

# x chat start ==> create a new thread
# x chat set 3 ==> set history to 3
# @gpt3 how to understand

# @ws /start
# @ws /set 3

# x chat history cp ''

# @en
# @cn

